{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9684764-1312-403c-86fe-1b1c23747997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.36.0-py3-none-any.whl.metadata (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.7.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m115.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf\n",
      "  Downloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
      "Collecting pyarrow>=8.0.0 (from datasets)\n",
      "  Downloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.25.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (15.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas->datasets)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m116.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading transformers-4.36.0-py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m154.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m120.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.7.0-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m151.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m117.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m199.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m188.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m140.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.7/225.7 kB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.5/502.5 kB\u001b[0m \u001b[31m144.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m118.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, pytz, xxhash, tzdata, tqdm, safetensors, regex, pyarrow-hotfix, pyarrow, protobuf, multidict, fsspec, frozenlist, et-xmlfile, dill, async-timeout, yarl, pandas, openpyxl, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, datasets, accelerate, peft\n",
      "Successfully installed accelerate-0.25.0 aiohttp-3.9.1 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.15.0 dill-0.3.7 et-xmlfile-1.1.0 frozenlist-1.4.0 fsspec-2023.10.0 huggingface-hub-0.19.4 multidict-6.0.4 multiprocess-0.70.15 openpyxl-3.1.2 pandas-2.1.4 peft-0.7.0 protobuf-4.25.1 pyarrow-14.0.1 pyarrow-hotfix-0.6 pytz-2023.3.post1 regex-2023.10.3 safetensors-0.4.1 sentencepiece-0.1.99 tokenizers-0.15.0 tqdm-4.66.1 transformers-4.36.0 tzdata-2023.3 xxhash-3.4.1 yarl-1.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers accelerate datasets peft sentencepiece protobuf openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f989d72e-bea4-4b3a-ad0d-e25bf0bfe89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c75890e4-d454-4ac9-b18a-7525677e6c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a522d4a-d15f-4b22-bd13-1ba195bf7a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>negative_response</th>\n",
       "      <th>postive_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#1. Post-Surgical Recovery: Patient mentions s...</td>\n",
       "      <td>#Trend 1 - Sleep\\n\\n#Trend 2 - Diet and BMI\\n\\...</td>\n",
       "      <td>#Trend 1 - Sleep\\n\\n#Trend 2 - Diet and BMI\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#1. Systemic Lupus Erythematosus: The patient ...</td>\n",
       "      <td>#Trend 1 - Sleep\\n\\n&gt;10. Sleep Disorders\\n\\n#T...</td>\n",
       "      <td>#Trend 1 - Sleep\\n\\n&gt;10. Sleep Disorders\\n\\n#T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#1. Systemic Lupus Erythematosus: The patient ...</td>\n",
       "      <td>#Trend 1 - Sleep\\n\\n#Trend 2 - Diet and BMI\\n\\...</td>\n",
       "      <td>#Trend 1 - Sleep\\n\\n#Trend 2 - Diet and BMI\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#1. Chronic Musculoskeletal Pain: The patient ...</td>\n",
       "      <td>#Trend 1 - Sleep\\n\\n#Trend 2 - Diet\\n\\n#Trend ...</td>\n",
       "      <td>#Trend 1 - Sleep\\n\\n#Trend 2 - Diet\\n\\n#Trend ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#1. Severe Social Anxiety Disorder: Verified b...</td>\n",
       "      <td>#Trend 1 - Sleep\\n\\n&gt;6. Sleep Disorder\\n\\n#Tre...</td>\n",
       "      <td>#Trend 1 - Sleep\\n\\n&gt;6. Sleep Disorder\\n\\n#Tre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>#1. Post radical prostatectomy complications: ...</td>\n",
       "      <td>#Trend 1 - Sleep\\n\\n#Trend 2 - Diet\\n\\n#Trend ...</td>\n",
       "      <td>#Trend 1 - Sleep\\n\\n#Trend 2 - Diet\\n\\n#Trend ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>#1. Prostate cancer - The patient mentions a P...</td>\n",
       "      <td>#Trend 1 - Sleep\\n\\n#Trend 2 - Diet\\n\\n#Trend ...</td>\n",
       "      <td>#Trend 1 - Sleep\\n\\n#Trend 2 - Diet\\n\\n#Trend ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>#1. Prostate cancer: The patient mentioned a s...</td>\n",
       "      <td>#Trend 1 - Sleep\\n\\n#Trend 2 - Diet\\n\\n#Trend ...</td>\n",
       "      <td>#Trend 1 - Sleep\\n\\n#Trend 2 - Diet\\n\\n#Trend ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>#1. Prostate Cancer: The patient mentioned a h...</td>\n",
       "      <td>#Trend 1 - Sleep\\n\\n#Trend 2 - Diet\\n\\n#Trend ...</td>\n",
       "      <td>#Trend 1 - Sleep\\n\\n#Trend 2 - Diet\\n\\n#Trend ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>#1. Post Prostatectomy Complications - The pat...</td>\n",
       "      <td>#Trend 1 - Sleep\\n\\n#Trend 2 - Diet\\n\\n#Trend ...</td>\n",
       "      <td>#Trend 1 - Sleep\\n\\n#Trend 2 - Diet\\n\\n#Trend ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>995 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prompt  \\\n",
       "0    #1. Post-Surgical Recovery: Patient mentions s...   \n",
       "1    #1. Systemic Lupus Erythematosus: The patient ...   \n",
       "2    #1. Systemic Lupus Erythematosus: The patient ...   \n",
       "3    #1. Chronic Musculoskeletal Pain: The patient ...   \n",
       "4    #1. Severe Social Anxiety Disorder: Verified b...   \n",
       "..                                                 ...   \n",
       "990  #1. Post radical prostatectomy complications: ...   \n",
       "991  #1. Prostate cancer - The patient mentions a P...   \n",
       "992  #1. Prostate cancer: The patient mentioned a s...   \n",
       "993  #1. Prostate Cancer: The patient mentioned a h...   \n",
       "994  #1. Post Prostatectomy Complications - The pat...   \n",
       "\n",
       "                                     negative_response  \\\n",
       "0    #Trend 1 - Sleep\\n\\n#Trend 2 - Diet and BMI\\n\\...   \n",
       "1    #Trend 1 - Sleep\\n\\n>10. Sleep Disorders\\n\\n#T...   \n",
       "2    #Trend 1 - Sleep\\n\\n#Trend 2 - Diet and BMI\\n\\...   \n",
       "3    #Trend 1 - Sleep\\n\\n#Trend 2 - Diet\\n\\n#Trend ...   \n",
       "4    #Trend 1 - Sleep\\n\\n>6. Sleep Disorder\\n\\n#Tre...   \n",
       "..                                                 ...   \n",
       "990  #Trend 1 - Sleep\\n\\n#Trend 2 - Diet\\n\\n#Trend ...   \n",
       "991  #Trend 1 - Sleep\\n\\n#Trend 2 - Diet\\n\\n#Trend ...   \n",
       "992  #Trend 1 - Sleep\\n\\n#Trend 2 - Diet\\n\\n#Trend ...   \n",
       "993  #Trend 1 - Sleep\\n\\n#Trend 2 - Diet\\n\\n#Trend ...   \n",
       "994  #Trend 1 - Sleep\\n\\n#Trend 2 - Diet\\n\\n#Trend ...   \n",
       "\n",
       "                                      postive_response  \n",
       "0    #Trend 1 - Sleep\\n\\n#Trend 2 - Diet and BMI\\n\\...  \n",
       "1    #Trend 1 - Sleep\\n\\n>10. Sleep Disorders\\n\\n#T...  \n",
       "2    #Trend 1 - Sleep\\n\\n#Trend 2 - Diet and BMI\\n\\...  \n",
       "3    #Trend 1 - Sleep\\n\\n#Trend 2 - Diet\\n\\n#Trend ...  \n",
       "4    #Trend 1 - Sleep\\n\\n>6. Sleep Disorder\\n\\n#Tre...  \n",
       "..                                                 ...  \n",
       "990  #Trend 1 - Sleep\\n\\n#Trend 2 - Diet\\n\\n#Trend ...  \n",
       "991  #Trend 1 - Sleep\\n\\n#Trend 2 - Diet\\n\\n#Trend ...  \n",
       "992  #Trend 1 - Sleep\\n\\n#Trend 2 - Diet\\n\\n#Trend ...  \n",
       "993  #Trend 1 - Sleep\\n\\n#Trend 2 - Diet\\n\\n#Trend ...  \n",
       "994  #Trend 1 - Sleep\\n\\n#Trend 2 - Diet\\n\\n#Trend ...  \n",
       "\n",
       "[995 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec559cfc-9eb9-48c3-a31a-ee5a60dad7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1. Post-Surgical Recovery: Patient mentions she is ten days post-surgery. \n",
      "#2. Possible Complications of Breast Augmentation: Based on her comment about wearing a bra with no wires for two months to help with scar healing, and her discussion of the 400s being \"really tight.\"\n",
      "#3. Skin Irritation or Allergic Reaction: The patient mentions burning, weird, and annoying sensations, suggesting possible skin irritation or an allergic reaction.\n",
      "#4. Post-Surgical Weight Fluctuation: The patient notes changes in her weight following surgery, suggesting a diagnosis related to post-surgical weight fluctuation.\n",
      "#5. Pain management and Opioid Withdrawal: She discusses reducing intake of pain pills over days, indicating potential issues related to pain management and possible opioid withdrawal.\n",
      "#6. Dehydration: The patient mentions drinking a gallon of water every day, suggesting potential dehydration.\n",
      "#7. Diet Management: She talks about keeping her diet high in protein and low in fat, indicating a diagnosis related to diet management.\n",
      "#8. Post-Surgery Dietary Adjustments: The patient's comment about eating \"whatever was in here pancakes\" and other foods due to the inability to cook suggests post-surgery dietary adjustments.\n",
      "#9. Calorie Intake and Diet Management: The mention of monitoring calorie intake and diet suggests a focus on calorie intake and diet management.\n",
      "#10. Breast Implant Displacement: Discusses starting to massage her implants to help them fall in place, suggesting a diagnosis related to potential breast implant displacement.\n",
      "#11. Post-Operative Muscular Limitations: Difficulty in \"putting the arms in complete motion\" suggests post-operative muscular limitations.\n",
      "#12. Potential Complications from Under the Muscle Implant Placement: The patient was told that some of the implants are over the muscle, indicating potential complications from under the muscle implant placement.\n",
      "#13. Post-Surgical Scarring: She references a need to avoid bras with wires so as to prevent scarring, suggesting a diagnosis related to post-surgical scarring.\n",
      "#14. Cardiovascular Activity Post-procedure: The doctor's advice to start slow cardio after 2 weeks implies a diagnosis relating to cardiovascular activity post-procedure.\n",
      "#15. Post-surgical Numbness: She describes her breasts as being \"very numb\" after surgery, indicating a diagnosis related to post-surgical numbness.\n",
      "#16. Delay in Resuming Physical Activities: She implies a delay in resuming physical activities, including no weight workouts and buddy lunches for body rehabilitation.\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0][\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0a96096-eab7-436a-a6e9-ecd995701e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Trend 1 - Sleep\n",
      "\n",
      "#Trend 2 - Diet and BMI\n",
      "\n",
      ">7. Diet Management\n",
      "\n",
      ">9. Calorie Intake and Diet Management\n",
      "\n",
      "#Trend 3 - Exercise\n",
      "\n",
      ">11. Post-Operative Muscular Limitations\n",
      "\n",
      ">14. Cardiovascular Activity Post-procedure\n",
      "\n",
      ">16. Delay in Resuming Physical Activities\n",
      "\n",
      "#Trend 4 - Mental Health\n",
      "\n",
      "#Trend 5 - Post-Surgical Recovery\n",
      "\n",
      ">1. Post-Surgical Recovery\n",
      "\n",
      ">5. Pain management\n",
      "\n",
      ">15. Post-surgical Numbness\n",
      "\n",
      "#Trend 6 - Breast Augmentation Complications\n",
      "\n",
      ">2. Possible Complications of Breast Augmentation\n",
      "\n",
      ">10. Breast Implant Displacement\n",
      "\n",
      ">12. Potential Complications from Under the Muscle Implant Placement\n",
      "\n",
      ">13. Post-Surgical Scarring\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0][\"postive_response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "876d5703-4b30-4e52-87bc-7dc7ec6eda55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from peft import get_peft_model, prepare_model_for_kbit_training\n",
    "# from utils.config_trainer import model_args, data_args, training_args\n",
    "# from utils.models import get_peft_config, get_quantization_config\n",
    "from transformers import (\n",
    "    set_seed,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "# from src_trainer.data_loader import load_dataset_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54015737-eee5-4c1f-a622-8bb654bae1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(\n",
    "    train_dataset_path: str,\n",
    "    test_dataset_path: str\n",
    ") -> DatasetDict:\n",
    "\n",
    "    raw_datasets = DatasetDict()\n",
    "    \n",
    "    raw_datasets[\"train\"] = load_dataset(\n",
    "      \"csv\",\n",
    "      data_files = train_dataset_path,\n",
    "      split=\"train\"\n",
    "    )\n",
    "    \n",
    "    raw_datasets[\"test\"] = load_dataset(\n",
    "      \"csv\",\n",
    "      data_files = test_dataset_path,\n",
    "      split=\"train\"\n",
    "    )\n",
    "    \n",
    "    return raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8c82da0-3ebc-41b0-ba3d-b99c24e17ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template(example, tokenizer):\n",
    "    \n",
    "    messages = [\n",
    "            {\n",
    "                \"role\":\"system\",\n",
    "                \"content\": SYSTEM_MESSAGE\n",
    "            },\n",
    "            {\n",
    "                \"content\": \"INPUT LIST OF DIAGNOSTIC HYPOTHESES:\\n\"+example[\"prompt\"],\n",
    "                \"role\":\"user\"\n",
    "            },\n",
    "            {\n",
    "                \"content\": example[\"postive_response\"] + \"\\n\\n###End\",\n",
    "                \"role\":\"assistant\"\n",
    "            }\n",
    "            ]\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3713c6a3-1360-488c-b77b-eaa8b4dca116",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_dataset(\n",
    "    \"train_df.csv\",\n",
    "    \"test_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e1ed65d-e02a-4695-aa04-ec304699106e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'negative_response', 'postive_response'],\n",
       "        num_rows: 995\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'negative_response', 'postive_response'],\n",
       "        num_rows: 110\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1360a195-b032-4a0a-ae24-7f14aa08b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"\n",
    "You will be given a list of diagnostic hypotheses from a doctor. Your job is to review and organize them into a structure that can later be turned into an in-depth report. The structure should adhere to the following rules:\n",
    "\n",
    "1. There must always be a trend for sleep, diet, exercise and mental health. Any health issues related to these default trends should be included in them. This true even if none of the diagnostic hypotheses fit into these default trends.\n",
    "\n",
    "2.  Trends that are very tightly related should be merged into a single trend. Everyone gets a Sleep trend, but if that patient also has sleep apnea, the trend would instead be a single Trend called \"Sleep And Sleep Apnea.\" A similar case would be for the Diet trend. If someone is overweight, instead of having a separate BMI trend, the unified Default Trend would be \"Diet and BMI.\" Likewise again, take the Default Trend of exercise, which everyone gets, if someone has long-standing injuries from exercise, this would be merged into the default Exercise trend to be \"Exercise and a History of Exercise Injuries.\" Anything directly related to Mental Health, such as depression, anxiety, stress, relationship issues, self-cutting, etc would all be under a single Default Trend called Mental Health regardless if it was preexisting or emerging.\n",
    "\n",
    "3. However just because two trends may seem superficially related doesn't mean they are. There must be a link that makes sense given the age, sex, body composition and medical history of the patient. Someone may have a history of Alzheimer's and be experiencing memory issues, but if they're only 20 years old, their memory loss is incredibly unlikely to be related to Alzheimer's. In this scenarios memory loss should be it's only separate trend. Likewise chest pain doesn't necessarily mean heart issues, even if there is a family history of heart issues. Chest pain can be respiratory, and if the patient has respiratory issue this must be balanced in. The appropriate response to this kind of ambiguity is to separate out ambiguous symptoms into their own trends. There must be a clear and logical link to blend multiple list elements under the same trend.\n",
    "\n",
    "4. Not all diagnostic hypothesis is high quality enough to make it to the report. There are no apparent symptoms or strong family history, the diagnostic hypothesis is likely too speculative.\n",
    "\n",
    "5. If a trend reflects an official diagnosis, then the trend name can reflect a diagnosis. If there isn't a diagnosis yet the trend name should reflect the symptoms reported, not a speculative diagnosis.\n",
    "\"\"\"\n",
    "\n",
    "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '### Input:\\n' + message['content'] }}\\n{% elif message['role'] == 'system' %}\\n{{ '### Instruction:' + message['content'] }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '\\n### Response:\\n'  + message['content'] }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '### Response:' }}\\n{% endif %}\\n{% endfor %}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "487ee1a4-e431-481d-8333-baaa659463fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(model_args):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "      model_args,\n",
    "      use_fast=False\n",
    "    )\n",
    "    \n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenizer.model_max_length = 4096\n",
    "    tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba3b2c37-2c2b-481a-913c-e209909a1eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"01-ai/Yi-34B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f96d581-ffee-40bb-913f-9af3c7206f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac7f02f0f7247d08c91d73643678f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/995 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235f6c5f25374dbd98d7490fc26c1fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = data.map(\n",
    "        apply_chat_template, \n",
    "        fn_kwargs={\"tokenizer\": tokenizer}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d726262-0331-422d-a21f-92aea2495c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'negative_response', 'postive_response', 'text'],\n",
       "        num_rows: 995\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'negative_response', 'postive_response', 'text'],\n",
       "        num_rows: 110\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a62ebb5-3d83-48e3-b4c4-a8756efb1ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_KEY = \" ### Response:\"\n",
    "\n",
    "class DataCollatorForCompletionOnlyLM(DataCollatorForLanguageModeling):\n",
    "    def torch_call(self, examples):\n",
    "        batch = super().torch_call(examples)\n",
    "        response_token_ids = self.tokenizer.encode(RESPONSE_KEY)\n",
    "        response_token_ids = response_token_ids[2:5]\n",
    "        labels = batch[\"labels\"].clone()\n",
    "        for i in range(len(examples)):\n",
    "            response_token_ids_start_idx = None\n",
    "            for idx in np.where(batch[\"labels\"][i] == response_token_ids[0])[0]:\n",
    "                if np.array_equal(\n",
    "                    response_token_ids,\n",
    "                    batch[\"labels\"][i, idx : idx + len(response_token_ids)],\n",
    "                ):\n",
    "                    response_token_ids_start_idx = idx\n",
    "                    break\n",
    "            if response_token_ids_start_idx is None:\n",
    "                raise RuntimeError(\"Could not find response key token IDs\")\n",
    "            response_token_ids_end_idx = response_token_ids_start_idx + len(\n",
    "                response_token_ids\n",
    "            )\n",
    "            labels[i, :response_token_ids_end_idx] = -100\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e23c2c07-0725-445f-918c-ac3432d6e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Tuple, Union\n",
    "\n",
    "def preprocess_batch(batch: Dict[str, List], tokenizer: AutoTokenizer, max_length: int = 4096) -> dict:\n",
    "  return tokenizer(\n",
    "      batch[\"text\"],\n",
    "      max_length=max_length,\n",
    "      truncation=True,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4daf6d5e-df5d-4ecf-8be3-10268992bac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359720499fc14df3b458362485cd0a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/995 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1695d1386f415395efaac8627455ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_preprocessing_function = partial(preprocess_batch, max_length=1024, tokenizer=tokenizer)\n",
    "data = data.map(\n",
    "    _preprocessing_function,\n",
    "    batched=True,\n",
    "    remove_columns=['prompt', 'negative_response', 'postive_response', \"text\"],\n",
    ")\n",
    "\n",
    "data = data.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a0af39c-3b5f-42d7-8e07-ed08abf055ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 995\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 110\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b3fb213-3002-4625-9597-da733a3ef5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "        tokenizer=tokenizer, mlm=False, return_tensors=\"pt\", pad_to_multiple_of=8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af7050f9-cf3f-49ae-8ea8-9841e96b28f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_lst = []\n",
    "for i in range(4):\n",
    "  main_lst.append(data['train'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d78649ad-75db-49b4-b678-bbdcc80c44c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  629,   562,  2376,  ...,  8308,  9395,   144],\n",
       "        [ 3037,  3691,   592,  ...,  8308,  9395,   144],\n",
       "        [  594,   972,    98,  ...,  8308,  9395,   144],\n",
       "        [27908,  1029,   562,  ...,  8308,  9395,   144]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[-100, -100, -100,  ..., 8308, 9395,  144],\n",
       "        [-100, -100, -100,  ..., 8308, 9395,  144],\n",
       "        [-100, -100, -100,  ..., 8308, 9395,  144],\n",
       "        [-100, -100, -100,  ..., 8308, 9395,  144]])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator.torch_call(main_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4c6e07-08cc-41d4-b0f2-581ab86979a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
