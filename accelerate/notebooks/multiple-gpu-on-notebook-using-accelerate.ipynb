{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-28T04:56:06.080974Z","iopub.status.busy":"2023-07-28T04:56:06.080171Z","iopub.status.idle":"2023-07-28T04:56:09.763789Z","shell.execute_reply":"2023-07-28T04:56:09.762672Z","shell.execute_reply.started":"2023-07-28T04:56:06.080939Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/summarize/rouge.py\n","/kaggle/input/summarize/clef_taskC_test3_full.json\n","/kaggle/input/summarize/train.json\n","/kaggle/input/summarize/valid.json\n","/kaggle/input/sec-edgar-companies-list/sec__edgar_company_info.csv\n","/kaggle/input/sec-edgar-companies-list/database.sqlite\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import torch\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T05:31:53.159966Z","iopub.status.busy":"2023-07-28T05:31:53.159564Z","iopub.status.idle":"2023-07-28T05:32:21.657117Z","shell.execute_reply":"2023-07-28T05:32:21.655562Z","shell.execute_reply.started":"2023-07-28T05:31:53.159934Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting accelerate==0.19\n","  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.19) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.19) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.19) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.19) (6.0)\n","Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.19) (2.0.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.19) (3.0.9)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.19) (3.12.2)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.19) (4.6.3)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.19) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.19) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.19) (3.1.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate==0.19) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate==0.19) (1.3.0)\n","Installing collected packages: accelerate\n","  Attempting uninstall: accelerate\n","    Found existing installation: accelerate 0.20.3\n","    Uninstalling accelerate-0.20.3:\n","      Successfully uninstalled accelerate-0.20.3\n","Successfully installed accelerate-0.19.0\n","Collecting rouge_score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.23.5)\n","Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n","Building wheels for collected packages: rouge_score\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24954 sha256=3abaf913428f3ed2b0d4d96ea51c049efff0ba11b5f87f15979e06cd119c7ea4\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge_score\n","Installing collected packages: rouge_score\n","Successfully installed rouge_score-0.1.2\n"]}],"source":["# !pip install transformers\n","!pip install accelerate\n","# !pip install datasets\n","!pip install rouge_score"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T05:32:21.659970Z","iopub.status.busy":"2023-07-28T05:32:21.659491Z","iopub.status.idle":"2023-07-28T05:32:21.666408Z","shell.execute_reply":"2023-07-28T05:32:21.665537Z","shell.execute_reply.started":"2023-07-28T05:32:21.659933Z"},"trusted":true},"outputs":[],"source":["max_target_length = 1024\n","\n","padding = \"max_length\"\n","\n","label_pad_token_id = -100\n","\n","weight_decay = 0.01\n","learning_rate = 0.0001\n","\n","gradient_accumulation_steps = 5\n","train_epoch = 10\n","\n","scheduler_type = \"linear\"\n","num_warmup_steps = 5\n","\n","output_dir = \"/kaggle/working/\""]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T05:35:19.616579Z","iopub.status.busy":"2023-07-28T05:35:19.616182Z","iopub.status.idle":"2023-07-28T05:35:19.656786Z","shell.execute_reply":"2023-07-28T05:35:19.655663Z","shell.execute_reply.started":"2023-07-28T05:35:19.616544Z"},"trusted":true},"outputs":[],"source":["def training_loop(seed: int = 42, batch_size: int = 8):\n","\n","    import transformers\n","    from accelerate import Accelerator\n","\n","    from huggingface_hub import Repository\n","    from transformers import (\n","        CONFIG_MAPPING,\n","        MODEL_MAPPING,\n","        AdamW,\n","        AutoConfig,\n","        AutoModelForSeq2SeqLM,\n","        AutoModelForMaskedLM,\n","        LEDTokenizer,\n","        LEDForConditionalGeneration,\n","        AutoTokenizer,\n","        AutoModel,\n","        EncoderDecoderConfig,\n","        LongformerTokenizer, \n","        EncoderDecoderModel,\n","        BartTokenizer,\n","        DataCollatorForSeq2Seq,\n","        SchedulerType,\n","        get_scheduler,\n","        set_seed\n","    )\n","    from transformers.file_utils import get_full_repo_name, is_offline_mode\n","    from transformers.utils.versions import require_version\n","    import torch\n","    from torch.utils.data import DataLoader\n","    import numpy as np\n","\n","    from datasets import load_dataset\n","    import math\n","    from datasets import load_metric\n","    import nltk\n","    from tqdm import tqdm\n","#     torch.multiprocessing.set_start_method('spawn')\n","\n","\n","    nltk.download(\"punkt\", quiet=True)\n","    # Initialize accelerator\n","    accelerator = Accelerator()\n","    dataset_name = \"cnn_dailymail\"\n","    raw_datasets = load_dataset(dataset_name)\n","    \n","    column_names = raw_datasets[\"train\"].column_names\n","\n","    text_column = column_names[0]\n","    summary_column = column_names[1]\n","\n","    \n","#     accelerate.print(\"AAA\")\n","\n","    set_seed(seed)\n","        \n","    tokenizer = BartTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n","    model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n","    print(\"*\"*100)\n","    \n","    def preprocess_function(examples):\n","        inputs = examples[text_column]\n","        targets = examples[summary_column]\n","\n","        model_inputs = tokenizer(inputs,\n","                               max_length= 1024,\n","                               padding=padding,\n","                               truncation=True)\n","\n","        with tokenizer.as_target_tokenizer():\n","            labels = tokenizer(targets,\n","                               max_length=max_target_length,\n","                               padding=padding,\n","                               truncation=True)\n","\n","        if padding==\"max_length\":\n","            labels[\"input_ids\"] = [\n","                [(l if l!=tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n","            ]\n","\n","        model_inputs[\"labels\"] = labels[\"input_ids\"]\n","        return model_inputs\n","    \n","    def postprocess_text(preds, labels):\n","        preds = [pred.strip() for pred in preds]\n","        labels = [label.strip() for label in labels]\n","\n","        # rougeLSum expects newline after each sentence\n","        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n","        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n","\n","        return preds, labels\n","    \n","    def get_dataloader(batch_size):\n","    \n","        with accelerator.main_process_first():\n","            processed_datasets = raw_datasets.map(\n","                      preprocess_function,\n","                      batched=True,\n","                      remove_columns=column_names,\n","                      desc=\"Running tokenizer on dataset\",\n","                  )\n","\n","\n","        train_dataset = processed_datasets[\"train\"]\n","        eval_dataset = processed_datasets[\"validation\"]\n","        test_dataset = processed_datasets[\"test\"]\n","\n","        data_collator = DataCollatorForSeq2Seq(\n","            tokenizer,\n","            model=model,\n","            label_pad_token_id=label_pad_token_id\n","        )\n","\n","        train_dataloader = DataLoader(\n","            train_dataset, shuffle=True, collate_fn=data_collator, batch_size=batch_size\n","        )\n","        eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=batch_size)\n","        test_dataloader = DataLoader(test_dataset, collate_fn=data_collator, batch_size=batch_size)\n","\n","        return train_dataloader, eval_dataloader, test_dataloader\n","    \n","    train_dataloader, eval_dataloader, test_dataloader = get_dataloader(batch_size=1)\n","    metric = load_metric(\"/kaggle/input/summarize/rouge.py\")\n","    print(\"*\"*100)\n","    # Optimizer\n","    # Split weights in two groups, one with weight decay and the other not.\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    \n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": weight_decay,\n","        },\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","            \"weight_decay\": 0.0,\n","        },\n","    ]\n","\n","    optimizer = AdamW(optimizer_grouped_parameters, lr= learning_rate)\n","    \n","    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n","\n","    max_train_steps = train_epoch * num_update_steps_per_epoch\n","\n","    lr_scheduler = get_scheduler(\n","            name=scheduler_type,\n","            optimizer=optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=max_train_steps,\n","        )\n","    \n","    print(\"*\"*100)\n","    model, optimizer, train_dataloader, eval_dataloader, test_dataloader, lr_scheduler = accelerator.prepare(\n","        model, optimizer, train_dataloader, eval_dataloader, test_dataloader, lr_scheduler)\n","    \n","    progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n","    completed_steps = 0\n","    best_val_score = 0\n","    best_epoch = -1\n","    print(\"*\"*100)\n","    for epoch in range(train_epoch):\n","        model.train()\n","\n","        for step, batch in enumerate(train_dataloader):\n","            outputs = model(**batch)\n","\n","            loss = outputs.loss\n","            loss = loss/gradient_accumulation_steps\n","\n","            accelerator.backward(loss)\n","\n","            if step % gradient_accumulation_steps == 0 or step == len(train_dataloader)-1:\n","                optimizer.step()\n","                lr_scheduler.step()\n","                optimizer.zero_grad()\n","                progress_bar.update(1)\n","                completed_steps +=1\n","\n","            if completed_steps >= max_train_steps:\n","                break\n","\n","        model.eval()\n","        val_max_target_length = max_target_length\n","\n","        gen_kwargs = {\n","          \"max_length\": val_max_target_length,\n","          \"num_beams\":4\n","        }\n","        if epoch>=0:\n","            to_dump = []\n","\n","            for step, batch in enumerate(eval_dataloader):\n","                with torch.no_grad():\n","\n","                    generated_tokens = accelerator.unwrap_model(model).generate(\n","                        batch[\"input_ids\"],\n","                        attention_mask = batch[\"attention_mask\"],\n","                        **gen_kwargs\n","                    )\n","\n","                    generated_tokens = accelerator.pad_across_processes(\n","                        generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n","                    )\n","\n","                    labels = batch[\"labels\"]\n","                    generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n","                    labels = accelerator.gather(labels).cpu().numpy()\n","\n","                    # Replace -100 in the labels as we can't decode them.(ignore pad for loss)\n","                    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","\n","                    if isinstance(generated_tokens, tuple):\n","                        generated_tokens = generated_tokens[0]\n","\n","                    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n","                    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","                    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","\n","                    metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n","\n","                    decoded_source = tokenizer.batch_decode(batch[\"input_ids\"],skip_special_tokens=True)\n","\n","                    for source,true,pred in zip(decoded_source,decoded_labels,decoded_preds):\n","                        sample={\"source\":source,\"true\":true,\"pred\":pred}\n","                        to_dump.append(sample)\n","\n","            with open(output_dir+\"prediction_epoch{}.json\".format(epoch),\"w\") as f:\n","                json.dump(to_dump, f,indent=4)\n","\n","            result = metric.compute(use_stemmer=True)\n","            # Extract a few results from ROUGE\n","            result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n","\n","            result = {k: round(v, 4) for k, v in result.items()}\n","\n","            if np.mean(list(result.values())) > best_val_score:\n","                best_val_score = np.mean(list(result.values()))\n","                best_epoch = epoch\n","\n","            print(f'At epoch {epoch}, the evaluation result is {result}, the best score is {best_val_score} at epoch {best_epoch}')\n","\n","        accelerator.wait_for_everyone()\n","        unwrapped_model = accelerator.unwrap_model(model)\n","        os.makedirs(output_dir+f'/{epoch}/', exist_ok=True)\n","        unwrapped_model.save_pretrained(output_dir+f'/{epoch}', save_function=accelerator.save)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T05:35:20.201484Z","iopub.status.busy":"2023-07-28T05:35:20.201126Z","iopub.status.idle":"2023-07-28T05:40:04.644344Z","shell.execute_reply":"2023-07-28T05:40:04.642868Z","shell.execute_reply.started":"2023-07-28T05:35:20.201455Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Launching training on 2 GPUs.\n","****************************************************************************************************\n","****************************************************************************************************\n","****************************************************************************************************\n","****************************************************************************************************\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["********************************************************************************************************************************************************************************************************\n","\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/140 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["****************************************************************************************************\n","****************************************************************************************************\n"]},{"name":"stderr","output_type":"stream","text":["  6%|▌         | 8/140 [00:26<07:10,  3.26s/it]"]}],"source":["# torch.multiprocessing.set_start_method('spawn')\n","\n","args = (42, 8)\n","import accelerate\n","accelerate.notebook_launcher(training_loop, args, num_processes=2)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
